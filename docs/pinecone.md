Here is a concise example of a Next.js 16 API route implementation using Pinecone with OpenAI's text-embedding-3-small model in Markdown format:

```md
# Next.js 16 + Pinecone + OpenAI text-embedding-3-small Example

## Setup

- Install dependencies:
```
npm install openai @pinecone-database/pinecone
```

- Set environment variables in `.env`:
```
OPENAI_API_KEY=your_openai_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=your_pinecone_env
PINECONE_INDEX=your_index_name
```

## API Route: /api/embeddings

```
import type { NextRequest } from 'next/server';
import { NextResponse } from 'next/server';
import OpenAI from 'openai';
import { PineconeClient } from '@pinecone-database/pinecone';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const pinecone = new PineconeClient();

await pinecone.init({
  apiKey: process.env.PINECONE_API_KEY ?? '',
  environment: process.env.PINECONE_ENVIRONMENT ?? '',
});

const index = pinecone.Index(process.env.PINECONE_INDEX ?? '');

export async function POST(req: NextRequest) {
  try {
    const { inputText, id } = await req.json();

    // Generate embeddings with OpenAI text-embedding-3-small
    const response = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: inputText,
    });

    const embedding = response.data.embedding;

    // Upsert embedding to Pinecone
    await index.upsert({
      upsertRequest: {
        vectors: [
          {
            id,
            values: embedding,
            metadata: { text: inputText },
          },
        ],
      },
    });

    return NextResponse.json({ message: 'Embedding stored', embedding }, { status: 200 });
  } catch (error) {
    console.error('Embedding error:', error);
    return NextResponse.json({ error: 'Failed to generate or store embedding' }, { status: 500 });
  }
}
```

## Usage Notes

- This route accepts POST requests with JSON body:
```
{
  "inputText": "Your text here",
  "id": "unique_vector_id"
}
```

- Embeddings are generated by OpenAI's `"text-embedding-3-small"` model and stored in Pinecone under the specified vector ID.

This minimal example can serve as a base for integrating Next.js 16 with Pinecone and OpenAI embeddings in a serverless environment.

Sources:

- Pinecone example usage and API [web:47][web:13]
- OpenAI embedding model `"text-embedding-3-small"` [web:12][web:13]
- Next.js API route conventions and Pinecone client usage [web:47]

[1](https://github.com/pinecone-io/embeddings-demo)
[2](https://www.pinecone.io/learn/openai-embeddings-v3/)
[3](https://cookbook.openai.com/examples/vector_databases/pinecone/using_pinecone_for_embeddings_search)
[4](https://www.youtube.com/watch?v=qpDDf0gvFag)
[5](https://docs.pinecone.io/models/text-embedding-3-small)
[6](https://cogniverse.net/guide-to-openai-embeddings-integration-with-langchain-pinecone/)
[7](https://www.djamware.com/post/6875b2b600a0506b9355826c/creating-a-personal-ai-assistant-with-langchain--pinecone--and-next-js)
[8](https://docs.pinecone.io/integrations/openai)
[9](https://www.youtube.com/watch?v=zIkHOrbpbCY)
[10](https://www.youtube.com/watch?v=KlZrcVSUpPQ)
[11](https://platform.openai.com/docs/models/text-embedding-3-small)
[12](https://www.youtube.com/watch?v=GgeoyzWBrSI)
[13](https://github.com/dabit3/semantic-search-nextjs-pinecone-langchain-chatgpt)
[14](https://www.youtube.com/watch?v=kVM_7m0iwrE)